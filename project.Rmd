---
title: "Predicting manner in which exercise is performed"
author: "Antony Samuel B"
date: "June 4, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

Data from accelerometers in the belt, forearm, arm and dumbell of 6 participants are obtained and using these the manner in which an 'Unilateral Dumbbell Biceps Curl is performed is studied. A randomforest with 10 - fold cross validation has been implemented after a principle component analysis on the prediction variables. The data provided for training has been split randomly into test and training sets for the purpose of cross validation. It can be seen that an average accuracy of 99% is achieved for the three cases looked at here. The focus here is to develop a prediction model that is scalable and accurate.

## Detailed report

#### Data loading and preprocessing

The test and training data sets are obtained from links [1](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [2](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv). It is seen that a number of columns have a lot of blank and missing (NA) values. The blank values are converted to NA when the data is loaded as shown below. Columns that have more than 95% of their values missing are removed. This way the dimensions of the data is reduced from 160 to 60.
```{r loadLib, message=FALSE, warning=FALSE, include=FALSE}
library(caret)
library(dplyr)
library(randomForest)
```
```{r loadData, echo = FALSE}
# loading data 
training <- read.csv("training.csv", na.strings=c("",".","NA"))
testing <- read.csv("testing.csv", na.strings=c("",".","NA"))
# removing predominantly NA columns
selNames <- apply(training, 2, function(x){ (1-sum(is.na(x))/length(x)) > 0.95} )
selNames <- names(training)[selNames]
training <- training[, selNames]
testing <- testing[, c(selNames[-60], "problem_id")]
```

```{r predVar, include=FALSE}
# grouping the predictor variables
trainPredVar <- training[,-c(1, 3:4)]
trainPredVar$cvtd_timestamp <- as.Date(as.character(trainPredVar$cvtd_timestamp),
                                       "%d/%m/%Y %H:%M")
testing$cvtd_timestamp <- as.Date(as.character(testing$cvtd_timestamp),
                                       "%d/%m/%Y %H:%M")
```

Splitting data into training and test datasets.
```{r splitData}
inTV <- createDataPartition(trainPredVar$roll_belt, p=0.8, list = FALSE)
trainingPV <- trainPredVar[inTV,]
testingPV <- trainPredVar[-inTV,]
```


#### Model Fitting

Since the original [study](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) has performed a random forest method to obtain predictions, the same method is followed here. Repeated K - fold cross validation available in the `caret` package is used for the `rf` method. First the model is run with default parameters for all predictors. Then the model is run with only ten trees per forest and then with only ten trees and 5 predictor variables. The 5 predictor variables are selected based on the important variables predicted by the first model.

```{r model1, cache=TRUE, message=FALSE, warning=FALSE}
# try 1: all parameters, default settings
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(.mtry= sqrt(ncol(trainingPV)))
modFitnew <- train(classe ~., data = trainingPV, method = "rf", 
                    metric = "Accuracy", 
                    tuneGrid=tunegrid, trControl=control)
print(modFitnew)
# prediction on split test case
prednew <- predict(modFitnew, testingPV)
print(sum(prednew == testingPV$classe)/length(prednew))
# 0.9966862
# importance of variables
impVar <- varImp(modFitnew)$importance
impVar$names <- row.names(impVar) 
impNames <- impVar[order(impVar$Overall, decreasing = TRUE),]$names
print(impNames[1:5])
```

The 5 most important variable according to the `rf` method is shown above. The predictions for the test data set provided is shown below.

```{r prediction}
# prediction on provided test case
predBM <- predict(modFitnew, testing)
print(predBM)
```

However this model takes a long time to execute. Hence a model where the number of trees per forest is set to 10 instead of the default 500 is used. The accuracy of this model is not that different from the previous one. Owing to space constraint the model is not shown and only the accuracy is shown.

```{r mod2, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(.mtry= sqrt(ncol(trainingPV)))
modFitTree10 <- train(classe ~., data = trainingPV, method = "rf", ntree = 10,
                   metric = "Accuracy", 
                   tuneGrid=tunegrid, trControl=control)

# prediction on split test case
prednewTree10 <- predict(modFitTree10, testingPV)
print(sum(prednewTree10 == testingPV$classe)/length(prednewTree10))
```

In third model attempt, ntree = 10 and the number of variables is taken to be only 5. The accuracy still remains high. Hence this model is used for predictions.

```{r mod3, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}
newDF5 <- trainingPV[, c(impNames[1:5], "classe")]
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(.mtry= 3)
modFitVar5 <- train(classe ~., data = newDF5, method = "rf", ntree = 10,
                      metric = "Accuracy", 
                      tuneGrid=tunegrid, trControl=control)
# prediction on split test case
prednewVar5 <- predict(modFitVar5, testingPV)
print(sum(prednewVar5 == testingPV$classe)/length(prednewVar5))
```
